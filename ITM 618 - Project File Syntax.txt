trainData=read.csv("trainset.csv",na="unknown")
View(trainData)

#Installing packages
install.packages("party")
install.packages("RWeka")
install.packages("caret")
install.packages("e1071")
install.packages("ROCR")
library(party)
library(RWeka)
library(caret)
library(ROCR)

#Cleaning Data - Removing NA/ Unknown variables from train & test Data
nrows <- nrow(trainData)
ncomplete <- sum(complete.cases(trainData))
ncomplete / nrows

#Visualize the missing data in the training dataset
gg_miss_var(trainData, show_pct = TRUE)


TrainingData=na.omit(trainData)
TrainingData$Subscribed=factor(TrainingData$Subscribed)
View(TrainingData)
testData <- read.csv("testset.csv", na="unknown")
Testingdata <- na.omit(testData)

ntrows <- nrow(testData)
ntcomplete <- sum(complete.cases(testData))
ntcomplete / ntrows


#Using One hot encoding to change the variables from categorical to numerical and Normalizing the variables

dumy <- dummyVars(" ~ .", data = TrainingData[,-15])
Finaldata <- data.frame(predict(dumy, newdata = TrainingData[,-15]))
Finaldata <- cbind(Finaldata,factor(TrainingData$Subscribed))
normalize <- function(x)
{
return((x- min(x)) /(max(x)-min(x)))
}

Finaldata$age <- normalize(Finaldata$age)
Finaldata$duration <- normalize(Finaldata$duration)
Finaldata$campaign <- normalize(Finaldata$campaign)
Finaldata$pdays <- normalize(Finaldata$pdays)
Finaldata$nr.employed <- normalize(Finaldata$nr.employed)
colnames(Finaldata)[51]="Subscribed"
View(Finaldata)

FinalTrainData <- Finaldata


dumy2 <- dummyVars(" ~ .", data = testData[,-15])
Finaldata2 <- data.frame(predict(dumy2, newdata = testData[,-15]))
Finaldata2 <- cbind(Finaldata2,factor(testData$Subscribed))

Finaldata2$age <- normalize(Finaldata2$age)
Finaldata2$duration <- normalize(Finaldata2$duration)
Finaldata2$campaign <- normalize(Finaldata2$campaign)
Finaldata2$pdays <- normalize(Finaldata2$pdays)
Finaldata2$nr.employed <- normalize(Finaldata2$nr.employed)
colnames(Finaldata2)[51]="Subscribed"
View(Finaldata2)

FinalTestData <- Finaldata2


#Use IG to see the best attributes to train the tree.
#Creating formula to use for IG
formula_all <- Subscribed ~ .
weights <- InfoGainAttributeEval(formula_all, data = FinalTrainData)
barplot(weights, las=3)


#creating Ctree with all of the attributes

cTree_1 <- ctree(formula_all, data = FinalTrainData)
plot(cTree_1)
savehistory()

#formula with the 3 best attributes found using the IG
formula_2 <- Subscribed ~ loanyes+ jobretired + poutcomesuccess
cTree_2 <- ctree(formula_2, data = FinalTrainData)
plot(cTree_2)


#TEST Trees 1 & 2

CTree_test1 <- predict(cTree_1, newdata = FinalTestData)
confusionMatrix(CTree_test1, FinalTestData$Subscribed)

CTree_test2 <- predict(cTree_2, newdata = FinalTestData)
confusionMatrix(CTree_test2, FinalTestData$Subscribed)



#Looking to make our linear regression model
str(FinalTrainData)
plot(FinalTrainData$Subscribed, FinalTrainData$poutcomesuccess, ylab = "Subscribed", xlab = "poutcomesuccess", main = "Successful Subscriptions?")

#Simple Linear Regression (SLR)
model1 <- lm(Subscribed ~ poutcomesuccess, data = FinalTrainData)
abline(model1)

model1


#Followed by a model using two attributes
model2 <- lm(Subscribed ~ poutcomesuccess + jobretired, data = FinalTrainData)
model2

#Ending with a model using three attributes
model3 <- lm(Subscribed ~ poutcomesuccess + jobretired + loanyes, data = FinalTrainData)
model3
model3pred=predict(model3,FinalTestData,type="response")


#Considered creating J48 Tree's to see if the accuracy increase
j48Tree_all <- J48(formula_all, data = FinalTrainData)
testJ48Tree_all <- predict(j48Tree_all, newdata = FinalTestData)
table(testJ48Tree_all, FinalTestData$Subscribed)
               

j48Tree_selected <- J48(formula_2, data = FinalTrainData)
testJ48Tree_selected <- predict(j48Tree_selected, newdata = FinalTestData)
table(testJ48Tree_selected, FinalTestData$Subscribed)

savehistory()                

